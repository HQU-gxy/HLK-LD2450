{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import Sequence, Any, Tuple, Union, List, Dict, Optional, TypeVar, Callable, Iterable, cast\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "data = ak.from_parquet(\"detections.parquet\")\n",
    "display(data.typestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_coords = data[:, :, [\"cX\", \"cY\"]]\n",
    "# use unzip to separate the x and y coordinates\n",
    "cXs, cYs = ak.unzip(detections_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rlabbe/filterpy\n",
    "import jax.numpy as jnp\n",
    "from jax import random, vmap, jit, grad, value_and_grad\n",
    "import numpy as np\n",
    "import jax\n",
    "import chex\n",
    "from jaxtyping import Array, Shaped, Num, Int, Float, Bool, PyTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class Initiator:\n",
    "    # tentative tracks are temporary tracks maintained by the initiator that\n",
    "    # have been initialized but not yet confirmed\n",
    "    tentative_tracks: Num[Array, \"... 2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filterpy\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "# input [x y]\n",
    "# state [x y dx/dt dy/dt]\n",
    "\n",
    "\n",
    "# yapf: disable\n",
    "def F_cv(dt: float|int):\n",
    "    return np.array([[1, 0, dt, 0],\n",
    "                     [0, 1, 0, dt],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "# yapf: enable\n",
    "\n",
    "\n",
    "# yapf: disable\n",
    "def H_cv():\n",
    "    return np.array([[1, 0, 0, 0], \n",
    "                     [0, 1, 0, 0]])\n",
    "# yapf: enable\n",
    "\n",
    "\n",
    "kf = KalmanFilter(4, 2)\n",
    "T = 1.0\n",
    "kf.F = F_cv(T)\n",
    "kf.H = H_cv()\n",
    "kf.R = np.diag([0.75, 0.75])\n",
    "kf.Q = np.diag([0.05, 0.05, 0.05, 0.05])\n",
    "# a simple constant velocity model\n",
    "# let's have a hypothesis of the initial velocity\n",
    "# is 0.05 unit/dt in both x and y directions\n",
    "kf.x = np.array([0, 0, 0.05, 0.05])\n",
    "display(kf.P)\n",
    "kf.predict()\n",
    "# x now becomes x prior\n",
    "display(kf.x)\n",
    "display(kf.P)\n",
    "\n",
    "# kf.update([0.15, 0.15])\n",
    "# x_posterior \n",
    "# when x is updated, it becomes x_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from predict state to measurement\n",
    "kf.x_prior # predicted state\n",
    "# predicted measurement\n",
    "# https://peps.python.org/pep-0465/\n",
    "predicted_measurement = kf.H @ kf.x\n",
    "display(predicted_measurement)\n",
    "# compare the predicted measurement with the actual measurement\n",
    "# with mahalanobis distance\n",
    "actual_measurement = np.array([0.12, 0.12])\n",
    "# or just euclidean distance\n",
    "kf.update(actual_measurement)\n",
    "display(kf.x)\n",
    "display(kf.P)\n",
    "display(kf.mahalanobis)\n",
    "# use mahalanobis distance as a loss function to determine the best match\n",
    "# Hungarian algorithm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html\n",
    "# when we get a successively detected object, we can move it into the confirmed tracks\n",
    "# and remove it from the tentative tracks\n",
    "# Well, it's more like two GNNs, one for the tentative tracks and one for the confirmed tracks\n",
    "# cascaded GNN, interesting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract out the Kalman filter\n",
    "# by motion model (state)\n",
    "# and measurement model\n",
    "\n",
    "# https://github.com/sisl/GaussianFilters.jl\n",
    "# not consider Input/External effect\n",
    "from jaxtyping import jaxtyped\n",
    "from typeguard import typechecked\n",
    "\n",
    "\n",
    "@chex.dataclass\n",
    "class LinearMotionNoInputModel:\n",
    "    F: Num[Array, \"n n\"]\n",
    "    Q: Num[Array, \"n n\"]\n",
    "\n",
    "\n",
    "@chex.dataclass\n",
    "class LinearMeasurementModel:\n",
    "    H: Num[Array, \"m n\"]\n",
    "    R: Num[Array, \"m m\"]\n",
    "\n",
    "\n",
    "Measurement = Num[Array, \"m\"]\n",
    "\n",
    "\n",
    "# a belief of Gaussian\n",
    "@chex.dataclass\n",
    "class GaussianState:\n",
    "    x: Num[Array, \"n\"]\n",
    "    P: Num[Array, \"n n\"]\n",
    "\n",
    "\n",
    "@jaxtyped(typechecker=typechecked)\n",
    "def _predict(\n",
    "    state: GaussianState,\n",
    "    motion_model: LinearMotionNoInputModel,\n",
    ") -> GaussianState:\n",
    "    x = state.x\n",
    "    P = state.P\n",
    "    F = motion_model.F\n",
    "    Q = motion_model.Q\n",
    "    assert x.shape[0] == F.shape[\n",
    "        0], \"state and transition model are not compatible\"\n",
    "    assert F.shape[0] == F.shape[1], \"transition model is not square\"\n",
    "    assert F.shape[0] == Q.shape[\n",
    "        0], \"transition model and noise model are not compatible\"\n",
    "    x_priori = F @ x\n",
    "    P_priori = F @ P @ F.T + Q\n",
    "    return GaussianState(x=x_priori, P=P_priori)\n",
    "\n",
    "\n",
    "@chex.dataclass\n",
    "class PosterioriResult:\n",
    "    # updated state\n",
    "    state: GaussianState\n",
    "    innovation: Num[Array, \"m\"]\n",
    "    posteriori_measurement: Num[Array, \"m\"]\n",
    "    mahalanobis_distance: Num[Array, \"m\"]\n",
    "    # post-fit residual\n",
    "    # y = z - H @ x_posteriori\n",
    "\n",
    "\n",
    "@jaxtyped(typechecker=typechecked)\n",
    "def update(\n",
    "    measurement: Measurement,\n",
    "    state: GaussianState,\n",
    "    measure_model: LinearMeasurementModel,\n",
    ") -> PosterioriResult:\n",
    "    x = state.x\n",
    "    P = state.P\n",
    "    H = measure_model.H\n",
    "    R = measure_model.R\n",
    "    assert x.shape[0] == H.shape[\n",
    "        1], \"state and measurement model are not compatible\"\n",
    "    assert H.shape[0] == R.shape[0], \"measurement model is not square\"\n",
    "    assert H.shape[0] == R.shape[1], \"measurement model is not square\"\n",
    "    z = measurement\n",
    "    inv = jnp.linalg.inv\n",
    "    # innovation\n",
    "    # the priori measurement residual\n",
    "    y = z - H @ x\n",
    "    # innovation covariance\n",
    "    S = H @ P @ H.T + R\n",
    "    # Kalman gain\n",
    "    K = P @ H.T @ inv(S)\n",
    "    # posteriori state\n",
    "    x_posteriori = x + K @ y\n",
    "    # dummy identity matrix\n",
    "    I = jnp.eye(P.shape[0])\n",
    "    # posteriori covariance\n",
    "    I_KH = I - K @ H\n",
    "    P_posteriori = I_KH @ P @ I_KH.T + K @ R @ K.T\n",
    "    posteriori_state = GaussianState(x=x_posteriori, P=P_posteriori)\n",
    "    posteriori_measurement = H @ x_posteriori\n",
    "    return PosterioriResult(\n",
    "        state=posteriori_state,\n",
    "        innovation=y,\n",
    "        posteriori_measurement=posteriori_measurement,\n",
    "        mahalanobis_distance=jnp.sqrt(y.T @ inv(S) @ y),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model(\n",
    "    v_x: float,\n",
    "    v_y: float,\n",
    "    dt: float,\n",
    "    q: float,\n",
    "    r: float,\n",
    ") -> Tuple[\n",
    "        LinearMotionNoInputModel,\n",
    "        LinearMeasurementModel,\n",
    "        GaussianState,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Create a constant velocity model with no input\n",
    "    \n",
    "    Args:\n",
    "    v_x: initial velocity in x direction\n",
    "    v_y: initial velocity in y direction\n",
    "    dt: time interval\n",
    "    q: process noise\n",
    "    r: measurement noise\n",
    "\n",
    "    Returns:\n",
    "    motion_model: motion model\n",
    "    measure_model: measurement model\n",
    "    state: initial state\n",
    "    \"\"\"\n",
    "    # yapf: disable\n",
    "    F = jnp.array([[1, 0, dt, 0],\n",
    "                        [0, 1, 0, dt],\n",
    "                        [0, 0, 1, 0],\n",
    "                        [0, 0, 0, 1]])\n",
    "    H = jnp.array([[1, 0, 0, 0],\n",
    "                        [0, 1, 0, 0]])\n",
    "    # yapf: enable\n",
    "    Q = q * jnp.eye(4)\n",
    "    R = r * jnp.eye(2)\n",
    "    P = jnp.eye(4)\n",
    "    motion_model = LinearMotionNoInputModel(F=F, Q=Q)\n",
    "    measure_model = LinearMeasurementModel(H=H, R=R)\n",
    "    state = GaussianState(x=jnp.array([0, 0, v_x, v_y]), P=P)\n",
    "    return motion_model, measure_model, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_model, me_model, st = cv_model(0.05, 0.05, 1.0, 0.05, 0.75)\n",
    "\n",
    "# predict\n",
    "new_st = _predict(st, mo_model)\n",
    "# update\n",
    "res = update(jnp.array([0.12, 0.12]), new_st, me_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_ak_record(dict_like: Dict[str, Any] | Any) -> ak.Record:\n",
    "#     return ak.Record(dict_like.__dict__)\n",
    "s = ak.Array([st.__dict__, new_st.__dict__])\n",
    "display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jaxtyped(typechecker=typechecked)\n",
    "def outer_distance(x: Num[Array, \"a 2\"], y: Num[Array,\n",
    "                                                \"b 2\"]) -> Num[Array, \"a b\"]:\n",
    "    \"\"\"\n",
    "    Here's equivalent python code:\n",
    "    \n",
    "    ```python\n",
    "    res = jnp.empty((x.shape[0], y.shape[0]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[0]):\n",
    "            # res[i, j] = jnp.linalg.norm(x[i] - y[j])\n",
    "            res = res.at[i, j].set(jnp.linalg.norm(x[i] - y[j]))\n",
    "    return res\n",
    "    ```\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    `outer product <https://en.wikipedia.org/wiki/Outer_product>`_\n",
    "    \"\"\"\n",
    "\n",
    "    @jit\n",
    "    def go(x, y):\n",
    "        x_expanded = x[:, None, :]\n",
    "        y_expanded = y[None, :, :]\n",
    "        diff = y_expanded - x_expanded\n",
    "        return jnp.linalg.norm(diff, axis=-1)\n",
    "\n",
    "    return go(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Generator, TypedDict\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html\n",
    "# https://github.com/google/jax/issues/10403\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "AKArray = ak.Array\n",
    "\n",
    "# register the JAX backend\n",
    "ak.jax.register_and_check()  # type: ignore\n",
    "\n",
    "\n",
    "@chex.dataclass\n",
    "class Tracking:\n",
    "    id: int\n",
    "    state: GaussianState\n",
    "    survived_time_steps: int\n",
    "    missed_time_steps: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrackerParams:\n",
    "    dt: float = 1.0\n",
    "    cov_threshold: float = 4.0\n",
    "    tentative_mahalanobis_threshold: float = 10.0\n",
    "    confirm_mahalanobis_threshold: float = 10.0\n",
    "    forming_tracks_euclidean_threshold: float = 25.0\n",
    "    survival_steps_threshold: int = 3\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    \"\"\"\n",
    "    A simple GNN tracker\n",
    "    \"\"\"\n",
    "    _last_measurements: Float[Array, \"... 2\"] = jnp.empty((0, 2),\n",
    "                                                          dtype=jnp.float32)\n",
    "    _tentative_tracks: list[Tracking] = []\n",
    "    _confirmed_tracks: list[Tracking] = []\n",
    "    _last_id: int = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self._last_measurements = jnp.array([], dtype=jnp.float32)\n",
    "        self._tentative_tracks = []\n",
    "        self._confirmed_tracks = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _predict(tracks: list[Tracking], dt: float = 1.0):\n",
    "        return [\n",
    "            Tracking(\n",
    "                id=track.id,\n",
    "                state=_predict(track.state, Tracker.motion_model(dt=dt)),\n",
    "                survived_time_steps=track.survived_time_steps,\n",
    "                missed_time_steps=track.missed_time_steps,\n",
    "            ) for track in tracks\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _data_associate_and_update(\n",
    "            measurements: Float[Array, \"... 2\"],\n",
    "            tracks: list[Tracking],\n",
    "            distance_threshold: float = 3) -> Float[Array, \"... 2\"]:\n",
    "        \"\"\"\n",
    "        Match tracks with measurements and update the tracks\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        [in] measurements: Float[\"a 2\"]\n",
    "        [in,out] tracks: Tracking[\"b\"]\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        return \n",
    "            Float[\"... 2\"] the unmatched measurements\n",
    "        \n",
    "        Effect\n",
    "        ----------\n",
    "        find the best match by minimum Mahalanobis distance, please note that I assume the state has been predicted\n",
    "        \"\"\"\n",
    "        if len(tracks) == 0:\n",
    "            return measurements\n",
    "\n",
    "        def _update(measurement: Float[Array, \"a 2\"], tracking: Tracking):\n",
    "            return update(measurement, tracking.state,\n",
    "                          Tracker.measurement_model())\n",
    "\n",
    "        def outer_posteriori(\n",
    "                measurements: Float[Array, \"a 2\"],\n",
    "                tracks: list[Tracking]) -> list[list[PosterioriResult]]:\n",
    "            \"\"\"\n",
    "            calculate the outer posteriori for each measurement and track\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            [in] measurements: Float[\"a 2\"]\n",
    "            [in] tracks: Tracking[\"b\"]\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            PosterioriResult[\"a b\"]\n",
    "            \"\"\"\n",
    "            return [[\n",
    "                _update(measurement, tracking) for measurement in measurements\n",
    "            ] for tracking in tracks]\n",
    "\n",
    "        def posteriori_to_mahalanobis(\n",
    "                posteriori: list[list[PosterioriResult]]\n",
    "        ) -> Float[Array, \"a b\"]:\n",
    "            \"\"\"\n",
    "            Parameters\n",
    "            ----------\n",
    "            [in] posteriori: PosterioriResult[\"a b\"]\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            Float[\"a b\"]\n",
    "            \"\"\"\n",
    "            return jnp.array(\n",
    "                [[r_m.mahalanobis_distance for r_m in p_t] for p_t in posteriori\n",
    "                ],\n",
    "                dtype=jnp.float32)\n",
    "\n",
    "        posteriors = outer_posteriori(measurements, tracks)\n",
    "        distances = posteriori_to_mahalanobis(posteriors)\n",
    "        row, col = linear_sum_assignment(np.array(distances))\n",
    "        row = jnp.array(row)\n",
    "        col = jnp.array(col)\n",
    "\n",
    "        def to_be_deleted() -> Generator[Tuple[int, int], None, None]:\n",
    "            for i, j in zip(row, col):\n",
    "                post: PosterioriResult = posteriors[i][j]\n",
    "                if post.mahalanobis_distance > distance_threshold:\n",
    "                    yield i, j\n",
    "\n",
    "        for i, j in to_be_deleted():\n",
    "            row = row[row != i]\n",
    "            col = col[col != j]\n",
    "\n",
    "        for i, j in zip(row, col):\n",
    "            track: Tracking = tracks[i]\n",
    "            post: PosterioriResult = posteriors[i][j]\n",
    "            track.state = post.state\n",
    "            track.survived_time_steps += 1\n",
    "            tracks[i] = track\n",
    "\n",
    "        for i, track in enumerate(tracks):\n",
    "            if i not in row:\n",
    "                # reset the survived time steps once missed\n",
    "                track.missed_time_steps += 1\n",
    "                tracks[i] = track\n",
    "        # remove measurements that have been matched\n",
    "        left_measurements = jnp.delete(measurements, col, axis=0)\n",
    "        return left_measurements\n",
    "\n",
    "    def _tracks_from_past_measurements(self,\n",
    "                                       measurements: Float[Array, \"... 2\"],\n",
    "                                       dt: float = 1.0,\n",
    "                                       distance_threshold: float = 3.0):\n",
    "        \"\"\"\n",
    "        consume the last measurements and create tentative tracks from them\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        mutate self._tentative_tracks and self._last_measurements\n",
    "        \"\"\"\n",
    "        if self._last_measurements.shape[0] == 0:\n",
    "            self._last_measurements = measurements\n",
    "            return\n",
    "        distances = outer_distance(self._last_measurements, measurements)\n",
    "        row, col = linear_sum_assignment(distances)\n",
    "        row = jnp.array(row)\n",
    "        col = jnp.array(col)\n",
    "\n",
    "        def to_be_deleted() -> Generator[Tuple[int, int], None, None]:\n",
    "            for i, j in zip(row, col):\n",
    "                euclidean_distance = distances[i, j]\n",
    "                if euclidean_distance > distance_threshold:\n",
    "                    yield i, j\n",
    "\n",
    "        for i, j in to_be_deleted():\n",
    "            row = row[row != i]\n",
    "            col = col[col != j]\n",
    "\n",
    "        for i, j in zip(row, col):\n",
    "            coord = measurements[j]\n",
    "            vel = (coord - self._last_measurements[i]) / dt\n",
    "            s = jnp.concatenate([coord, vel])\n",
    "            state = GaussianState(x=s, P=jnp.eye(4))\n",
    "            track = Tracking(id=self._last_id,\n",
    "                             state=state,\n",
    "                             survived_time_steps=0,\n",
    "                             missed_time_steps=0)\n",
    "            self._last_id += 1\n",
    "            self._tentative_tracks.append(track)\n",
    "        # update the last measurements with the unmatched measurements\n",
    "        self._last_measurements = jnp.delete(measurements, col, axis=0)\n",
    "\n",
    "    def _transfer_tentative_to_confirmed(self,\n",
    "                                        survival_steps_threshold: int = 3):\n",
    "        \"\"\"\n",
    "        transfer tentative tracks to confirmed tracks\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        mutate self._tentative_tracks and self._confirmed_tracks in place\n",
    "        \"\"\"\n",
    "        for i, track in enumerate(self._tentative_tracks):\n",
    "            if track.survived_time_steps > survival_steps_threshold:\n",
    "                self._confirmed_tracks.append(track)\n",
    "                self._tentative_tracks.pop(i)\n",
    "\n",
    "    @staticmethod\n",
    "    def _track_cov_deleter(tracks: list[Tracking], cov_threshold: float = 4.0):\n",
    "        \"\"\"\n",
    "        delete tracks with covariance trace greater than threshold\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        [in,out] tracks: list[Tracking]\n",
    "        cov_threshold: float\n",
    "            the threshold of the covariance trace\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        mutate tracks in place\n",
    "        \"\"\"\n",
    "        for i, track in enumerate(tracks):\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.trace.html\n",
    "            if jnp.trace(track.state.P) > cov_threshold:\n",
    "                tracks.pop(i)\n",
    "\n",
    "    def next_measurements(self, measurements: Float[Array, \"... 2\"],\n",
    "                          params: TrackerParams):\n",
    "        self._confirmed_tracks = self._predict(self._confirmed_tracks,\n",
    "                                               params.dt)\n",
    "        self._tentative_tracks = self._predict(self._tentative_tracks,\n",
    "                                               params.dt)\n",
    "        left_ = self._data_associate_and_update(\n",
    "            measurements, self._confirmed_tracks,\n",
    "            params.confirm_mahalanobis_threshold)\n",
    "        left = self._data_associate_and_update(\n",
    "            left_, self._tentative_tracks,\n",
    "            params.tentative_mahalanobis_threshold)\n",
    "        self._transfer_tentative_to_confirmed(params.survival_steps_threshold)\n",
    "        self._tracks_from_past_measurements(\n",
    "            left, params.dt, params.forming_tracks_euclidean_threshold)\n",
    "        self._track_cov_deleter(self._tentative_tracks, params.cov_threshold)\n",
    "        self._track_cov_deleter(self._confirmed_tracks, params.cov_threshold)\n",
    "\n",
    "    @property\n",
    "    def confirmed_tracks(self):\n",
    "        return self._confirmed_tracks\n",
    "\n",
    "    @staticmethod\n",
    "    def motion_model(dt: float = 1,\n",
    "                     q: float = 0.05) -> LinearMotionNoInputModel:\n",
    "        \"\"\"\n",
    "        a constant velocity motion model\n",
    "        \"\"\"\n",
    "        # yapf: disable\n",
    "        F = jnp.array([[1, 0, dt, 0],\n",
    "                            [0, 1, 0, dt],\n",
    "                            [0, 0, 1, 0],\n",
    "                            [0, 0, 0, 1]])\n",
    "        # yapf: enable\n",
    "        Q = q * jnp.eye(4)\n",
    "        return LinearMotionNoInputModel(F=F, Q=Q)\n",
    "\n",
    "    @staticmethod\n",
    "    def measurement_model(r: float = 0.75) -> LinearMeasurementModel:\n",
    "        # yapf: disable\n",
    "        H = jnp.array([[1, 0, 0, 0],\n",
    "                            [0, 1, 0, 0]])\n",
    "        # yapf: enable\n",
    "        R = r * jnp.eye(2)\n",
    "        return LinearMeasurementModel(H=H, R=R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_measurements() -> Generator[Float[Array, \"... 2\"], None, None]:\n",
    "    for m_cXs, m_cYs in zip(cXs, cYs):\n",
    "        nxs = m_cXs.to_numpy()\n",
    "        nys = m_cYs.to_numpy()\n",
    "        xs = jnp.array(nxs)\n",
    "        ys = jnp.array(nys)\n",
    "        yield jnp.column_stack([xs, ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker()\n",
    "\n",
    "tenative_histories: list[list[Tracking]] = []\n",
    "confirmed_histories: list[list[Tracking]] = []\n",
    "\n",
    "params = TrackerParams(\n",
    "    cov_threshold=25.0,\n",
    "    tentative_mahalanobis_threshold=50.0,\n",
    "    confirm_mahalanobis_threshold=25.0,\n",
    "    forming_tracks_euclidean_threshold=20,\n",
    "    dt=1.0,\n",
    "    survival_steps_threshold=6,\n",
    ")\n",
    "\n",
    "for measurement in gen_measurements():\n",
    "    m = jnp.array(measurement)\n",
    "    tracker.next_measurements(m, params)\n",
    "    tenative_histories.append(tracker._tentative_tracks.copy())\n",
    "    confirmed_histories.append(tracker._confirmed_tracks.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import cv2\n",
    "from cv2.typing import MatLike\n",
    "from loguru import logger\n",
    "\n",
    "@dataclass\n",
    "class CapProps:\n",
    "    width: int\n",
    "    height: int\n",
    "    fps: float\n",
    "    frame_count: Optional[int] = None\n",
    "\n",
    "def fourcc(*args: str) -> int:\n",
    "    return cv2.VideoWriter_fourcc(*args)  # type: ignore\n",
    "\n",
    "def video_cap(\n",
    "        src: str | int) -> Tuple[Generator[MatLike, None, None], CapProps]:\n",
    "    cap = cv2.VideoCapture(src)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = float(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    props = CapProps(width, height, fps, frame_count)\n",
    "\n",
    "    def gen():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            yield frame\n",
    "        cap.release()\n",
    "\n",
    "    return gen(), props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, props = video_cap(\"PETS09-S2L1-raw.mp4\")\n",
    "writer = cv2.VideoWriter(\"PETS09-S2L1-tracking.mp4\",\n",
    "                         fourcc(*\"mp4v\"),\n",
    "                         props.fps, (props.width, props.height),\n",
    "                         isColor=True)\n",
    "\n",
    "class RawDataDict(TypedDict):\n",
    "    x: int\n",
    "    y: int\n",
    "    w: int\n",
    "    h: int\n",
    "    area: float\n",
    "    cX: int\n",
    "    cY: int\n",
    "\n",
    "display(props)\n",
    "\n",
    "try:\n",
    "    colors = np.random.randint(0, 255, size=(1024, 3))\n",
    "    for frame, tentative_tracks, confirmed_tracks, raws in zip(\n",
    "            frames, tenative_histories, confirmed_histories, data): # type: ignore\n",
    "        for raw in raws:\n",
    "            x, y, w, h, area, cX, cY = raw.x, raw.y, raw.w, raw.h, raw.area, raw.cX, raw.cY\n",
    "            cv.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # generate a random color map for each track\n",
    "        for track in tentative_tracks:\n",
    "            x, y = track.state.x[:2]\n",
    "            color_ = colors[track.id]\n",
    "            color = tuple(color_.tolist())\n",
    "            # cv.rectangle(frame, (int(x - 5), int(y - 5), 10, 10), color, -1)\n",
    "        for track in confirmed_tracks:\n",
    "            x, y = track.state.x[:2]\n",
    "            color_ = colors[track.id]\n",
    "            color = tuple(color_.tolist())\n",
    "            cv.circle(frame, (int(x), int(y)), 5, color, -1)\n",
    "        writer.write(frame)\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "finally:\n",
    "    writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula you provided is used to calculate the probabilities of different hypotheses in a data association problem, specifically in the context of the Probabilistic Data Association Filter (PDAF). Let's break it down:\n",
    "\n",
    "## Hypothesis Probabilities\n",
    "\n",
    "The probabilities for the hypotheses are calculated as follows:\n",
    "\n",
    "$$\n",
    "\\beta_i(k) = \\begin{cases}\n",
    "  \\frac{\\mathcal{L}_{i}(k)}{1-P_{D}P_{G}+\\sum_{j=1}^{m(k)} \\mathcal{L}_{j}(k)}, \\quad i=1,...,m(k) \\\\\n",
    "  \\frac{1-P_{D}P_{G}}{1-P_{D}P_{G}+\\sum_{j=1}^{m(k)} \\mathcal{L}_{j}(k)}, \\quad i=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $\\beta_i(k)$ represents the probability of the $i$-th hypothesis at time step $k$.\n",
    "- $i=0$ corresponds to the missed detection hypothesis, where none of the detections are associated with the track.\n",
    "- $i=1,...,m(k)$ corresponds to the true detection hypotheses, where detection $i$ is associated with the track.\n",
    "- $P_D$ is the detection probability.\n",
    "- $P_G$ is the gate probability.\n",
    "- $\\mathcal{L}_{i}(k)$ is the likelihood ratio of the measurement $z_{i}(k)$ originating from the track target rather than clutter.\n",
    "\n",
    "## Likelihood Ratio\n",
    "\n",
    "The likelihood ratio $\\mathcal{L}_{i}(k)$ is calculated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{i}(k) = \\frac{\\mathcal{N}[z_{i}(k);\\hat{z}(k|k-1), S(k)]P_{D}}{\\lambda}\n",
    "$$\n",
    "\n",
    "- $\\mathcal{N}[z_{i}(k);\\hat{z}(k|k-1), S(k)]$ is the likelihood of the measurement $z_{i}(k)$ given the predicted measurement $\\hat{z}(k|k-1)$ and the innovation covariance matrix $S(k)$.\n",
    "- $\\lambda$ is the clutter density.\n",
    "\n",
    "## Normalization\n",
    "\n",
    "The probabilities are normalized such that they sum to 1. Since all probabilities have the same denominator, it can be discarded during the calculation and the probabilities can be normalized later.\n",
    "\n",
    "## Gating\n",
    "\n",
    "The function also includes a gating step to validate measurements. The gating threshold is calculated using the chi-square distribution:\n",
    "\n",
    "$$\n",
    "\\text{gate threshold} = \\chi^2_{\\text{prob gate}, n}\n",
    "$$\n",
    "\n",
    "- $\\text{prob gate}$ is the desired gating probability.\n",
    "- $n$ is the dimension of the measurement space.\n",
    "\n",
    "Measurements that fall outside the gating threshold are considered invalid and may be excluded from the association process unless the `include_all` flag is set.\n",
    "\n",
    "## Validation Region Volume\n",
    "\n",
    "The validation region volume is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{validation region volume} = c_z \\cdot \\text{gate threshold}^{n/2} \\cdot \\sqrt{\\det(S(k))}\n",
    "$$\n",
    "\n",
    "- $c_z = \\frac{\\pi^{n/2}}{\\Gamma(\\frac{n}{2} + 1)}$ is the volume constant.\n",
    "- $S(k)$ is the innovation covariance matrix.\n",
    "\n",
    "The validation region volume is used to adjust the probabilities of the true detection hypotheses when the clutter spatial density is not provided.\n",
    "\n",
    "This formula is a key component of the PDAF algorithm, which helps in associating measurements with tracks in the presence of clutter and missed detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Probabilities\n",
    "\n",
    "In the PDAF algorithm, there are multiple hypotheses considered at each time step:\n",
    "\n",
    "1. The \"missed detection\" hypothesis ($i=0$) assumes that none of the detections are associated with the target being tracked. \n",
    "\n",
    "2. The \"true detection\" hypotheses ($i=1,...,m(k)$) assume that detection $i$ is associated with the target being tracked.\n",
    "\n",
    "The probabilities of these hypotheses, denoted as $\\beta_i(k)$, indicate how likely each hypothesis is based on the observed measurements and the predicted target state. They are calculated using a formula that takes into account the detection probability ($P_D$), the gate probability ($P_G$), and the likelihood ratios of the measurements.\n",
    "\n",
    "## Likelihood Ratio\n",
    "\n",
    "The likelihood ratio, denoted as $\\mathcal{L}_{i}(k)$, represents the likelihood of a measurement $z_{i}(k)$ originating from the target being tracked rather than from clutter or false alarms. It is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{i}(k) = \\frac{\\mathcal{N}[z_{i}(k);\\hat{z}(k|k-1), S(k)]P_{D}}{\\lambda}\n",
    "$$\n",
    "\n",
    "Here, $\\mathcal{N}[z_{i}(k);\\hat{z}(k|k-1), S(k)]$ represents the probability density of the measurement $z_{i}(k)$ given the predicted measurement $\\hat{z}(k|k-1)$ and the innovation covariance matrix $S(k)$. $P_D$ is the detection probability, and $\\lambda$ is the clutter density[1][4].\n",
    "\n",
    "Intuitively, the likelihood ratio compares how well the observed measurement matches the predicted measurement based on the target's estimated state. A higher likelihood ratio indicates a higher probability that the measurement came from the target rather than from clutter.\n",
    "\n",
    "The hypothesis probabilities and likelihood ratios are used in the PDAF algorithm to calculate a weighted average of the validated measurements, which is then used to update the target's state estimate. This allows the algorithm to handle situations where there is uncertainty in the association between measurements and the target being tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Probability-generating_function\n",
    "https://cran.r-project.org/web/packages/dst/vignettes/Introduction_to_Belief_Functions.html\n",
    "https://www.youtube.com/watch?v=0b82A29t1yM&list=PLadnyz93xCLhFinI8NO30-1e6SwCGRTIM&index=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFS: Random Finite Sets\n",
    "\n",
    "PPP: Poisson Point Process\n",
    "\n",
    "https://stackoverflow.com/questions/31133232/poisson-point-process-in-python-3-with-numpy-without-scipy\n",
    "https://hpaulkeeler.com/poisson-point-process-simulation/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardinality of RFS is poisson distributed\n",
    "\n",
    "Po(n; lambda_bar)\n",
    "\n",
    "A PPP is a specific type of RFS model. An RFS is a random set that contains a finite but random number of elements. A PPP is a particular kind of RFS where the number of points follows a Poisson distribution, and the points are independently and uniformly distributed in the space.\n",
    "\n",
    "- clutter detection \n",
    "- appearing objects\n",
    "- measurements from extended objects\n",
    "\n",
    "Bernoulli RFS (or Bernoulli Process) The Bernoulli RFS is a special case of the Poisson RFS where the number of elements is either 0 or 1. This corresponds to the presence or absence of a single object in the set.\n",
    "\n",
    "- measurement of a single object\n",
    "- a potential object\n",
    "\n",
    "Multi-Bernoulli RFS, union of independent Bernoulli RFS\n",
    "\n",
    "use convolution formula to calculate multi-object PDF\n",
    "\n",
    "- model potential objects, according to the posterior\n",
    "- appearing objects\n",
    "\n",
    "independent but not necessary identically distributed compared to PPP\n",
    "\n",
    "MB almost equivalent to PPP\n",
    "\n",
    "- Bernoulli RFS with $r < 0.1$ is approximately PPP\n",
    "- a MB with $r_1,\\ldots,r_N < 0.1$ is approximately PPP\n",
    "- Any PPP can be approximated by a MB, but it may require a large $N$\n",
    "\n",
    "PPP is often computationally more efficient than MB\n",
    "\n",
    "Why use MB instead of PPP?\n",
    "\n",
    "- mean and variance of $|\\mathcal{X}|$ are all $\\bar{\\lambda}$\n",
    "- We know the number of objects we're looking for, which is large, but don't want make the variance equal to the mean ($\\bar{\\lambda}$)\n",
    "- Have precise information about the distribution (could use different distributions for different regions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Bernoulli Mixture (MBM) RFS\n",
    "\n",
    "state of art in multi-target tracking\n",
    "\n",
    "Categorical Distribution\n",
    "\n",
    "$h \\sim \\text{Cat}(w)$ if\n",
    "\n",
    "$$\\text{Pr}[h=j] = w_j$$\n",
    "\n",
    "Sometimes it's easier to generate [Multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution)\n",
    "\n",
    "Multiple hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Filtering with RFS\n",
    "\n",
    "Champman-Kolmogorov equation for prediction and Bayes' rule for update\n",
    "\n",
    "No idea what's Chapman-Kolmogorov equation\n",
    "\n",
    "A motion and a measurement model\n",
    "\n",
    "state now is a set, not only capturing how the state evolves but also how the cardinality evolves (object birth, death)\n",
    "\n",
    "convolution formula (integral of RFS?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motion model: number of object is time invariant and using RFS (the cardinality)\n",
    "Measurement model doesn't really *changed*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
