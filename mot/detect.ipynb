{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Generator, Iterable, List, Optional, Tuple, TypedDict, cast\n",
    "\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from cv2 import BackgroundSubtractor, BackgroundSubtractorKNN, BackgroundSubtractorMOG2\n",
    "from cv2.typing import MatLike, Size\n",
    "from loguru import logger\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import PathLike\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CapProps:\n",
    "    width: int\n",
    "    height: int\n",
    "    channels: int\n",
    "    fps: float\n",
    "    frame_count: Optional[int] = None\n",
    "\n",
    "\n",
    "def video_cap(\n",
    "    src: PathLike | int,\n",
    "    scale: float = 1,\n",
    ") -> Tuple[Generator[MatLike, None, None], CapProps]:\n",
    "    assert 0 < scale <= 1, \"scale should be in (0, 1]\"\n",
    "    if isinstance(src, PathLike):\n",
    "        cap = cv2.VideoCapture(str(src))\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(src)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * scale)\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * scale)\n",
    "    fps = float(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    channels = int(cap.get(cv2.CAP_PROP_CHANNEL))\n",
    "    props = CapProps(width=width,\n",
    "                     height=height,\n",
    "                     fps=fps,\n",
    "                     channels=channels,\n",
    "                     frame_count=frame_count)\n",
    "\n",
    "    def gen():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if scale != 1:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "            yield frame\n",
    "        cap.release()\n",
    "\n",
    "    return gen(), props\n",
    "\n",
    "\n",
    "def fourcc(*args: str) -> int:\n",
    "    return cv2.VideoWriter_fourcc(*args)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-09 11:19:50.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mVideo properties: CapProps(width=768, height=576, channels=0, fps=7.0, frame_count=795)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6a775f237a41ac9020ef66cb66ae09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/795 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import awkward as ak\n",
    "\n",
    "VIDEO_BG_PATH = Path(\"PETS09-S2L1-raw.mp4\")\n",
    "frames, props = video_cap(VIDEO_BG_PATH)\n",
    "logger.info(f\"Video properties: {props}\")\n",
    "is_mono = props.channels == 1\n",
    "bg_writer = cv2.VideoWriter(\"PETS09-S2L1-bgsub.mp4\",\n",
    "                            fourcc(*\"mp4v\"),\n",
    "                            props.fps, (props.width, props.height),\n",
    "                            isColor=False)\n",
    "writer = cv2.VideoWriter(\"PETS09-S2L1-detection.mp4\", fourcc(*\"mp4v\"),\n",
    "                         props.fps, (props.width, props.height))\n",
    "\n",
    "# learningRate\n",
    "# The value between 0 and 1 that indicates how fast the background model is\n",
    "# learnt. Negative parameter value makes the algorithm to use some automatically\n",
    "# chosen learning rate. 0 means that the background model is not updated at all,\n",
    "# 1 means that the background model is completely reinitialized from the last\n",
    "# frame.\n",
    "subtractor = cv2.createBackgroundSubtractorMOG2(detectShadows=False)\n",
    "\n",
    "\n",
    "class DetectionFeatures(TypedDict):\n",
    "    x: int\n",
    "    y: int\n",
    "    w: int\n",
    "    h: int\n",
    "    area: float\n",
    "    cX: int\n",
    "    cY: int\n",
    "\n",
    "\n",
    "# * means variable length (0 or more)\n",
    "# frame count is determined by the video\n",
    "# [frame detection* features]\n",
    "batch_detection = ak.Array([])\n",
    "\n",
    "try:\n",
    "    for frame in tqdm(frames, total=props.frame_count):\n",
    "        fgmask = subtractor.apply(frame)\n",
    "        contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL,\n",
    "                                       cv2.CHAIN_APPROX_SIMPLE)\n",
    "        detections = ak.Array([])\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 100:\n",
    "                M = cv2.moments(contour)\n",
    "                cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                features: DetectionFeatures = {\n",
    "                    \"x\": x,\n",
    "                    \"y\": y,\n",
    "                    \"w\": w,\n",
    "                    \"h\": h,\n",
    "                    \"area\": area,\n",
    "                    \"cX\": cX,\n",
    "                    \"cY\": cY\n",
    "                }\n",
    "                ak_features = ak.Array([features])\n",
    "                detections = ak.concatenate([detections, ak_features])\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.circle(frame, (cX, cY), 5, (0, 0, 255), -1)\n",
    "        batch_detection = ak.concatenate(\n",
    "            [batch_detection, ak.Array([detections])])\n",
    "        bg_writer.write(fgmask)\n",
    "        writer.write(frame)\n",
    "except KeyboardInterrupt as e:\n",
    "    bg_writer.release()\n",
    "    writer.release()\n",
    "    raise e\n",
    "finally:\n",
    "    bg_writer.release()\n",
    "    writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'795 * var * {x: int64, y: int64, w: int64, h: int64, area: float64, cX: int64, cY: int64}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_detection = cast(ak.Array, batch_detection)\n",
    "batch_detection.typestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# save the batch_detection (as akward array)\n",
    "# https://github.com/scikit-hep/awkward/discussions/329\n",
    "aw = ak.to_arrow_table(batch_detection)\n",
    "pq.write_table(aw, \"detections.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'795 * var * {x: int64, y: int64, w: int64, h: int64, area: float64, cX: int64, cY: int64}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to read the parquet file\n",
    "table = pq.read_table(\"detections.parquet\")\n",
    "ak.from_arrow(table).typestr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
